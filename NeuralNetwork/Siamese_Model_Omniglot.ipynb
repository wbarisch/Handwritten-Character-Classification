{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from PIL import Image\n",
    "import random\n",
    "from dataset import OmniglotDataset, kWay_nShotDataset\n",
    "from train_funcs import trainSiamese,testSiamese_kway_nshot\n",
    "from saving import save_checkpoint, load_checkpoint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_static, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Downloading Omniglot dataset')\n",
    "root = \"/Data\"\n",
    "Omniglot(root=root, background=True, download=True, transform=transforms.ToTensor())\n",
    "Omniglot(root=root, background=False, download=True, transform=transforms.ToTensor())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = \"/data/omniglot-py/images_background\"\n",
    "test_root = \"/data/omniglot-py/images_evaluation\"\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomAffine(degrees=15,translate=(0.1,0.1),scale=(0.9, 1.1), fill=255),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = OmniglotDataset(train_root,30000*8,transform=data_transforms)\n",
    "val_dataset = OmniglotDataset(train_root,10000,transform=data_transforms)\n",
    "test_dataset = kWay_nShotDataset(test_root,500,kway=40,nshot=5,transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SiameseModel,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            #1 @ 105x105\n",
    "            nn.Conv2d(in_channels=1,out_channels=64,kernel_size=10),\n",
    "            nn.ReLU(),\n",
    "            #64 @ 96x96\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            \n",
    "            #64 @ 48x48\n",
    "            nn.Conv2d(in_channels=64,out_channels=128,kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            #128 @ 42x42\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            \n",
    "            #128 @ 21x21\n",
    "            nn.Conv2d(in_channels=128,out_channels=128,kernel_size=4),\n",
    "            nn.ReLU(),\n",
    "            #128 @ 18x18\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            \n",
    "            #128 @ 9x9\n",
    "            nn.Conv2d(in_channels=128,out_channels=256,kernel_size=4),\n",
    "            nn.ReLU(),\n",
    "            #256 @ 6x6\n",
    "        )\n",
    "        \n",
    "        # HCC-129: Add dropout layer to model\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(4096, 1)\n",
    "        \n",
    "    def calculateEmbedding(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1,256 * 6 * 6)\n",
    "        x = self.sig(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self,x1,x2):\n",
    "        x = torch.abs(x1 - x2)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseModel()\n",
    "\n",
    "print(f\"Model Architecture: {model.cuda()}\\n\")\n",
    "print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "EPOCHS = 500\n",
    "learning_rate = 0.00008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "total_training_time = 0.0\n",
    "print_rate=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Training Started\\n\")\n",
    "for i in range(EPOCHS):\n",
    "    \n",
    "    now = time.time()\n",
    "    \n",
    "    train_loss, val_loss = trainSiamese(\n",
    "                                        model=model,\n",
    "                                        train_dataloader=train_loader,\n",
    "                                        val_dataloader=val_loader,\n",
    "                                        optimizer=optimizer,\n",
    "                                        criterion=criterion)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_time = (end - now) / 60\n",
    "    \n",
    "    total_training_time += epoch_time \n",
    "    \n",
    "    train_loss_history.append(train_loss)\n",
    "    val_loss_history.append(val_loss)\n",
    "    print(f\"Epoch {i+1}/{EPOCHS}\")\n",
    "    if i % print_rate == 0:\n",
    "        print(f\"Epoch {i+1}/{EPOCHS}, Train_Loss: {train_loss:.4f}, Val_Loss: {val_loss:.4f}\")\n",
    "        print(f\"Time for Epoch({i+1}): {epoch_time:.2f} Minutes\\n\")\n",
    "        save_checkpoint(i,model=model,optimizer=optimizer,train_loss_history=train_loss_history,val_loss_history=val_loss_history)\n",
    "    \n",
    "print(f\"Total training time: {total_training_time} Minutes\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model = SiameseModel()\n",
    "model.cuda()\n",
    "train_loss_history, val_loss_history = load_checkpoint(\"500_checkpoint_08_11.pt\",model=model,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss_history, label='Train Loss', color='blue', marker='o')\n",
    "plt.plot(val_loss_history, label='Validation Loss', color='darkorange', marker='o')\n",
    "\n",
    "# Adding labels, legend, and grid\n",
    "plt.title('Train and Validation Loss History', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot Train Loss\n",
    "axes[0].plot(train_loss_history, label='Train Loss', color='blue', marker='o')\n",
    "axes[0].set_title('Training Loss History', fontsize=16)\n",
    "axes[0].set_xlabel('Epochs', fontsize=14)\n",
    "axes[0].set_ylabel('Loss', fontsize=14)\n",
    "axes[0].legend(fontsize=12)\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot Validation Loss\n",
    "axes[1].plot(val_loss_history, label='Validation Loss', color='darkorange', marker='o')\n",
    "axes[1].set_title('Validation Loss History', fontsize=16)\n",
    "axes[1].set_xlabel('Epochs', fontsize=14)\n",
    "axes[1].set_ylabel('Loss', fontsize=14)\n",
    "axes[1].legend(fontsize=12)\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "The model will be tested over 500 samples in 40-way 5-shot recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = testSiamese_kway_nshot(model,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load The Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseModel()\n",
    "model.cuda()\n",
    "load_checkpoint(\"500_checkpoint_08_11.pt\",model=model,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the embedding model in a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(EmbeddingWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.calculateEmbedding(x)\n",
    "\n",
    "\n",
    "embedding_model = EmbeddingWrapper(model)\n",
    "embedding_model = embedding_model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input_image = torch.randn(1, 1, 105, 105).cuda() \n",
    "\n",
    "torch.onnx.export(\n",
    "    embedding_model,                \n",
    "    dummy_input_image,                        \n",
    "    \"siamese_embedding_model_500.onnx\",                   \n",
    "    input_names=[\"input_image\"],              \n",
    "    output_names=[\"embedding\"],               \n",
    "    dynamic_axes={\"input_image\": {0: \"batch_size\"}, \"embedding\": {0: \"batch_size\"}},  \n",
    "    opset_version=11\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Comparison Model (Not used in final app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_embedding1 = torch.randn(1, 4096).cuda()  \n",
    "dummy_embedding2 = torch.randn(1, 4096).cuda()  \n",
    "\n",
    "torch.onnx.export(\n",
    "    model,                                \n",
    "    (dummy_embedding1, dummy_embedding2),  \n",
    "    \"siamese_comparison_model_500.onnx\",       \n",
    "    input_names=[\"embedding1\", \"embedding2\"],  \n",
    "    output_names=[\"output\"],               \n",
    "    dynamic_axes={\n",
    "        \"embedding1\": {0: \"batch_size\"},   \n",
    "        \"embedding2\": {0: \"batch_size\"},   \n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    },\n",
    "    opset_version=11\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize Embedding Model (Not used in final App)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, dataloader, num_batches):\n",
    "\n",
    "        self.dataloader = iter(dataloader)\n",
    "        self.num_batches = num_batches\n",
    "        self.batch_count = 0\n",
    "        self.data = None\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.batch_count < self.num_batches:\n",
    "            try:\n",
    "                img1, _ , _ = next(self.dataloader)\n",
    "                \n",
    "                img1_np = img1.numpy() \n",
    "                \n",
    "                self.data = {\"input_image\": img1_np}\n",
    "                self.batch_count += 1\n",
    "                return self.data\n",
    "            except StopIteration:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def rewind(self):\n",
    "        self.dataloader = iter(self.dataloader)\n",
    "        self.batch_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "calibration_data_reader = SiameseCalibrationDataReader(dataloader=train_loader, num_batches=10)\n",
    "\n",
    "onnx_model_path = \"siamese_embedding_model_500.onnx\"\n",
    "quantized_model_path = \"siamese_embedding_model_500_quantized.onnx\"\n",
    "\n",
    "quantize_static(\n",
    "    model_input=onnx_model_path,\n",
    "    model_output=quantized_model_path,\n",
    "    calibration_data_reader=calibration_data_reader,\n",
    "    weight_type=QuantType.QInt8\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
